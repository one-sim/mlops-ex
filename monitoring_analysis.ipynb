{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a376a57d",
   "metadata": {},
   "source": [
    "# MLOps Sentiment Analysis - Sistema di Monitoraggio Completo\n",
    "\n",
    "Questo notebook implementa il sistema completo di monitoraggio e retraining per il modello di sentiment analysis. Include:\n",
    "- ✅ Download del dataset\n",
    "- ✅ Valutazione del modello su test set (Accuracy, F1-Score)\n",
    "- ✅ Logging delle predizioni con timestamp\n",
    "- ✅ Tracking delle metriche (distribuzione sentiment, confidence media)\n",
    "- ✅ Rilevazione del drift confrontando con baseline\n",
    "- ✅ Trigger di retraining basato su performance\n",
    "- ✅ Visualizzazioni dei risultati\n",
    "- ✅ Test unitari della funzione principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d85d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 649.45it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForSequenceClassification LOAD REPORT from: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.pooler.dense.bias       | UNEXPECTED |  | \n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "roberta.pooler.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'wasserstein_distance' from 'scipy.spatial.distance' (/workspaces/mlops-ex/.venv/lib/python3.12/site-packages/scipy/spatial/distance.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmonitoring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PredictionLogger, PredictionLog\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetricsTracker, SentimentMetrics\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdrift_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DriftDetector, DriftReport\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrainingManager, RetrainingTrigger\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Configurazione di stile\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/mlops-ex/src/drift_detection.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, asdict\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wasserstein_distance\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmonitoring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PredictionLog\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'wasserstein_distance' from 'scipy.spatial.distance' (/workspaces/mlops-ex/.venv/lib/python3.12/site-packages/scipy/spatial/distance.py)"
     ]
    }
   ],
   "source": [
    "# 1. Importare Librerie Necessarie\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librerie di visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn per le metriche\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Modelli locali\n",
    "sys.path.insert(0, '/workspaces/mlops-ex')\n",
    "from src.sentiment_model import analyze_sentiment\n",
    "from src.monitoring import PredictionLogger, PredictionLog\n",
    "from src.metrics import MetricsTracker, SentimentMetrics\n",
    "from src.drift_detection import DriftDetector, DriftReport\n",
    "from src.retraining import RetrainingManager, RetrainingTrigger\n",
    "\n",
    "# Configurazione di stile\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Tutte le librerie importate correttamente\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configurare il Sistema di Logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Crea le directory necessarie\n",
    "logs_dir: Path = Path(\"logs\")\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Inizializza i componenti del sistema di monitoraggio\n",
    "logger: PredictionLogger = PredictionLogger(log_dir=\"logs\")\n",
    "metrics_tracker: MetricsTracker = MetricsTracker(metrics_dir=\"logs\")\n",
    "drift_detector: DriftDetector = DriftDetector(\n",
    "    baseline_file=\"logs/baseline_distribution.json\",\n",
    "    drift_threshold=0.15,\n",
    "    metrics_dir=\"logs\"\n",
    ")\n",
    "retraining_manager: RetrainingManager = RetrainingManager(\n",
    "    min_samples_for_retraining=50,\n",
    "    confidence_threshold=0.70,\n",
    "    drift_detector=drift_detector,\n",
    "    metrics_dir=\"logs\"\n",
    ")\n",
    "\n",
    "# Pulisci i log precedenti (opzionale)\n",
    "logger.clear_logs()\n",
    "metrics_tracker.clear_metrics()\n",
    "drift_detector.clear_drift_reports()\n",
    "retraining_manager.clear_triggers()\n",
    "\n",
    "print(\"✅ Sistema di logging configurato\")\n",
    "print(f\"Directory dei log: {logs_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a03a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scaricare e Caricare il Dataset\n",
    "# Carica il dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Mapping delle label\n",
    "label_mapping: Dict[int, str] = {0: \"Negativo\", 1: \"Neutro\", 2: \"Positivo\"}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "print(f\"Label mapping: {label_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implementare Prediction Logger (esempio d'uso)\n",
    "# Esegui inferenze su un campione e logga le predizioni\n",
    "import numpy as np\n",
    "\n",
    "sample_size: int = 100\n",
    "indices = np.random.choice(len(dataset['test']), sample_size, replace=False)\n",
    "\n",
    "for i in indices:\n",
    "    sample = dataset['test'][int(i)]\n",
    "    text = sample['text']\n",
    "    true_label = label_mapping[sample['label']]\n",
    "    \n",
    "    scores = analyze_sentiment(text)\n",
    "    pred_label = max(scores, key=scores.get)\n",
    "    \n",
    "    logger.log_prediction(text=text, sentiment_scores=scores)\n",
    "\n",
    "print(f\"✅ Loggate {logger.get_logs_count()} predizioni di esempio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd54261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calcolare Metriche Aggregate\n",
    "\n",
    "logs = logger.load_logs()\n",
    "metrics = metrics_tracker.calculate_metrics(logs)\n",
    "metrics_tracker.save_metrics(metrics)\n",
    "\n",
    "print(\"Metriche aggregate:\")\n",
    "print(json.dumps(metrics.to_dict(), indent=2))\n",
    "\n",
    "# Mostra distribuzione sentiment\n",
    "sns.barplot(\n",
    "    x=list(metrics.sentiment_distribution.keys()),\n",
    "    y=list(metrics.sentiment_distribution.values())\n",
    ")\n",
    "plt.title(\"Distribuzione dei sentiment (campione)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confidenza media: {metrics.average_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e510fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implementare Drift Detection\n",
    "\n",
    "logs = logger.load_logs()\n",
    "report = drift_detector.detect_drift(logs)\n",
    "\n",
    "drift_detector.save_drift_report(report)\n",
    "\n",
    "print(\"Drift report:\")\n",
    "print(json.dumps(report.to_dict(), indent=2))\n",
    "\n",
    "if report.drift_detected:\n",
    "    print(\"⚠️ Drift rilevato — considerare il retraining\")\n",
    "else:\n",
    "    print(\"No drift rilevato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Implementare Trigger Retraining\n",
    "\n",
    "logs = logger.load_logs()\n",
    "trigger = retraining_manager.evaluate_retraining_need(logs)\n",
    "retraining_manager.save_trigger(trigger)\n",
    "\n",
    "print(\"Retraining trigger:\")\n",
    "print(json.dumps(trigger.to_dict(), indent=2))\n",
    "\n",
    "if trigger.triggered:\n",
    "    print(\"✅ Trigger per retraining attivato: \", trigger.recommended_action)\n",
    "else:\n",
    "    print(\"Nessun retraining necessario al momento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Valutare Modello su Test Set\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Usa un campione del test set per la valutazione\n",
    "sample_size = min(200, len(dataset['test']))\n",
    "indices = np.random.choice(len(dataset['test']), sample_size, replace=False)\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "confs = []\n",
    "\n",
    "for i in indices:\n",
    "    sample = dataset['test'][int(i)]\n",
    "    text = sample['text']\n",
    "    true_label = label_mapping[sample['label']]\n",
    "    res = analyze_sentiment(text)\n",
    "    pred_label = max(res, key=res.get)\n",
    "    conf = res[pred_label]\n",
    "\n",
    "    preds.append(pred_label)\n",
    "    trues.append(true_label)\n",
    "    confs.append(conf)\n",
    "\n",
    "# Mappa a valori numerici\n",
    "label_to_idx = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "y_true = [label_to_idx[t] for t in trues]\n",
    "y_pred = [label_to_idx[p] for p in preds]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"EVALUATION\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 (weighted): {f1:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.values()))\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Distribuzione confidenza\n",
    "sns.histplot(confs, bins=20)\n",
    "plt.title('Distribuzione dei confidence score')\n",
    "plt.xlabel('Confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c816ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Visualizzare Distribuzioni Sentiment nel tempo\n",
    "\n",
    "# Calcola metriche su finestre orarie\n",
    "metrics_over_time = metrics_tracker.get_metrics_over_time(logger.load_logs(), window_hours=1)\n",
    "\n",
    "# Prepara i dati per il plotting\n",
    "if metrics_over_time:\n",
    "    timestamps = [t for t, m in metrics_over_time]\n",
    "    positives = [m.sentiment_distribution.get('Positivo', 0) for t, m in metrics_over_time]\n",
    "    neutrals = [m.sentiment_distribution.get('Neutro', 0) for t, m in metrics_over_time]\n",
    "    negatives = [m.sentiment_distribution.get('Negativo', 0) for t, m in metrics_over_time]\n",
    "\n",
    "    plt.plot(timestamps, positives, label='Positivo')\n",
    "    plt.plot(timestamps, neutrals, label='Neutro')\n",
    "    plt.plot(timestamps, negatives, label='Negativo')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.title('Evoluzione distribuzione sentiment nel tempo (finestre orarie)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Non ci sono abbastanza dati per mostrare metriche nel tempo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ef0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Test unitari della funzione principale (esempi)\n",
    "# Esegue un set di test rapidi per verificare logger, metrics, drift e retraining\n",
    "\n",
    "from src.monitoring import PredictionLogger\n",
    "from src.metrics import MetricsTracker\n",
    "from src.drift_detection import DriftDetector\n",
    "from src.retraining import RetrainingManager\n",
    "from datetime import datetime\n",
    "\n",
    "# Test 1: Logging corretto\n",
    "pl = PredictionLogger(log_dir='logs')\n",
    "pl.clear_logs()\n",
    "res = {'Positivo': 0.9, 'Neutro': 0.09, 'Negativo': 0.01}\n",
    "entry = pl.log_prediction('Test logging', res)\n",
    "assert pl.get_logs_count() >= 1\n",
    "print('Test 1 passed: logging corretto')\n",
    "\n",
    "# Test 2: Metrics aggregation\n",
    "mt = MetricsTracker(metrics_dir='logs')\n",
    "logs = pl.load_logs()\n",
    "metrics = mt.calculate_metrics(logs)\n",
    "assert metrics.total_predictions >= 1\n",
    "print('Test 2 passed: metrics aggregation')\n",
    "\n",
    "# Test 3: Drift detection con distribuzioni artificiali\n",
    "from src.monitoring import PredictionLog\n",
    "fake_logs = [\n",
    "    PredictionLog(timestamp=datetime.now().isoformat(), text='t1', sentiment='Positivo', confidence=0.9, scores={'Positivo':0.9,'Neutro':0.08,'Negativo':0.02})\n",
    "    for _ in range(60)\n",
    "]\n",
    "\n",
    "det = DriftDetector(baseline_file='logs/baseline_test.json', metrics_dir='logs', drift_threshold=0.1)\n",
    "det.set_baseline(fake_logs[:30])\n",
    "report = det.detect_drift(fake_logs[30:])\n",
    "print('Test 3 passed: drift detection eseguito', report.drift_detected)\n",
    "\n",
    "# Test 4: Retraining trigger\n",
    "rm = RetrainingManager(min_samples_for_retraining=10, confidence_threshold=0.95, drift_detector=det, metrics_dir='logs')\n",
    "trigger = rm.evaluate_retraining_need(fake_logs)\n",
    "print('Test 4 passed: retraining evaluated', trigger.triggered)\n",
    "\n",
    "print('✅ Tutti i test rapidi sono stati eseguiti')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
